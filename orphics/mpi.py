from __future__ import print_function
import numpy as np
import os,sys,time
from contextlib import contextmanager
import traceback

"""
Copied to pyfisher
"""

try:
    disable_mpi_env = os.environ['DISABLE_MPI']
    disable_mpi = True if disable_mpi_env.lower().strip() == "true" else False
except:
    disable_mpi = False

"""
Use the below cleanup stuff only for intel-mpi!
If you use it on openmpi, you will have no traceback for errors
causing hours of endless confusion and frustration! - Sincerely, past frustrated Mat
"""
# From Sigurd's enlib.mpi:
# Uncaught exceptions don't cause mpi to abort. This can lead to thousands of
# wasted CPU hours
# def cleanup(type, value, traceback):
#     sys.__excepthook__(type, value, traceback)
#     MPI.COMM_WORLD.Abort(1)
# sys.excepthook = cleanup


@contextmanager
def mpi_abort_on_exception(comm):
    try:
        yield
    except Exception as e:
        if comm.Get_rank() == 0:
            print(f"Exception: {e}", file=sys.stderr)
            traceback.print_exc()
        comm.Abort(1)

class fakeMpiComm:
    """
    A Simple Fake MPI implementation
    """
    def __init__(self):
        self.size = self.Get_size()
        self.rank = self.Get_rank()
    def Get_rank(self):
        return 0
    def Get_size(self):
        return 1
    def Barrier(self):
        pass
    def Abort(self,dummy):
        pass
    def allgatherv(self,x):
        return x




try:
    if disable_mpi: raise
    from mpi4py import MPI
except:
    if not(disable_mpi): print("WARNING: mpi4py could not be loaded. Falling back to fake MPI. This means that if you submitted multiple processes, they will all be assigned the same rank of 0, and they are potentially doing the same thing.")


    class template:
        pass

    MPI = template()
    MPI.COMM_WORLD = fakeMpiComm()
    


    
def mpi_distribute(num_tasks,avail_cores,allow_empty=False):
    # copied to mapsims.convert_noise_templates
    if not(allow_empty): assert avail_cores<=num_tasks
    min_each, rem = divmod(num_tasks,avail_cores)
    num_each = np.array([min_each]*avail_cores) # first distribute equally
    if rem>0: num_each[-rem:] += 1  # add the remainder to the last set of cores (so that rank 0 never gets extra jobs)

    task_range = list(range(num_tasks)) # the full range of tasks
    cumul = np.cumsum(num_each).tolist() # the end indices for each task
    task_dist = [task_range[x:y] for x,y in zip([0]+cumul[:-1],cumul)] # a list containing the tasks for each core
    assert sum(num_each)==num_tasks
    assert len(num_each)==avail_cores
    assert len(task_dist)==avail_cores
    return num_each,task_dist
    


def distribute(njobs,verbose=True,**kwargs):
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    numcores = comm.Get_size()
    num_each,each_tasks = mpi_distribute(njobs,numcores,**kwargs)
    if rank==0: print ("At most ", max(num_each) , " tasks...")
    my_tasks = each_tasks[rank]
    return comm,rank,my_tasks


class MPIDict(object):

    def __init__(self,init_dict,comm):
        self.rank = comm.Get_rank()
        self.numcores = comm.Get_size()
        self.comm = comm
        if self.rank==0:
            self.d = init_dict
        else:
            self.s = {}
        
    def update(self,key,value):
        if self.rank==0:
            self.d[key] = value
        else:
            self.s[key] = value
    def collect(self):
        if self.rank!=0:
            self.comm.send(self.s,dest=0,tag=self.rank)
            return None
        else:
            for i in range(1,self.numcores):
                s = self.comm.recv(source=i,tag=i)
                for key in s.keys():
                    assert key not in self.d.keys()
                    self.d[key] = s[key].copy()
            return self.d
            
    


    

        

### SCINET JOBMAKER
"""
Not technically MPI and kind of legacy.
"""

class jobMaker:

    '''
    Use this to send jobs to SciNet GPC in batches of numCores
    '''
    

    def __init__(self,projectName,walltime,commandPreFix="",numCores=8,queue='debug',jobRoot=None):
        

        self.numCores = numCores
        self.queue = queue
        self._headerText = "#!/bin/bash\n"+\
            "# MOAB/Torque submission script for multiple serial jobs on\n"+\
            "# SciNet GPC automatically generated by gpcInterface.jobMaker().\n"+\
            "#\n"+\
            "#PBS -l nodes=1:ppn=8,walltime="
        self._headerText2 = "\n#PBS -N "
        self._headerText3 = "\n#PBS -q " 
        self._headerText4 = "\n# DIRECTORY TO RUN - $PBS_O_WORKDIR is directory job was submitted from\n"+\
            "cd $PBS_O_WORKDIR\n"
        
        self.name = projectName
        self.walltime = walltime

        self._header = self._headerText + self.walltime + self._headerText2 + self.name + self._headerText3 + self.queue + self._headerText4

        self.jobScript = self._header

        if commandPreFix!="":
            self.prefix = commandPreFix+"; "
        else:
            self.prefix = commandPreFix

        self.scripts = []
        self._jobCount = 0
        self.submittedJobIDs = []

        if jobRoot==None:
            self._jobRoot = "jobs/"
        else:
            self._jobRoot = jobRoot
        self._jobLog = open(jobRoot+"jobs.log",'a')
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime())
        self._jobLog.write("\nStarted jobMaker object named =="+self.name+"== at "+timestamp)

    def addJob(self,command):

        self._jobCount+=1
        self.jobScript = self.jobScript+"\n("+self.prefix+command+") &"

        if self._jobCount%self.numCores==0:
            self.scripts.append(self.jobScript + "\n\nwait\n")
            self.jobScript = self._header

    def submit(self):
        import re

        if self._jobCount%self.numCores!=0:
            self.scripts.append(self.jobScript + "\n\nwait\n")
            self.jobScript = self._header

        self.jobScript = self._header

        j=0
        for script in self.scripts:
            j+=1
            filename = self._jobRoot+self.name+str(time.time())+str(j)+".sh"
            with open(filename,'w') as tempFile:
                tempFile.write(script)

            # print script
            # continue
            #sys.exit()    
            ##suboutput = os.popen('python labs/testthis.py').read()
            suboutput = os.popen('qsub '+filename).read()
            print (suboutput)
            jobid = int(re.findall('\d+', suboutput)[0])
            self.submittedJobIDs.append(jobid)

            timestamp = time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime())
            self._jobLog.write("\nOutput from script submitted at  "+timestamp+":\n"+suboutput)

            


        print (self._jobCount, "job(s) submitted in", j, "script(s).")
        
        self.scripts = []
        self._jobCount = 0



    
    def __del__(self):
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime())
        self._jobLog.write("\nClosed jobMaker object named =="+self.name+"== at "+timestamp)
        self._jobLog.close()
